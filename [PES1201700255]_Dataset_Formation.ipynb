{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "[PES1201700255] Dataset Formation.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT2nUA-95zWF",
        "colab_type": "text"
      },
      "source": [
        "# Creating a Dataset using Pheme \n",
        "\n",
        "**The dataset can be got form:** https://figshare.com/articles/PHEME_dataset_for_Rumour_Detection_and_Veracity_Classification/6392078"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llJ7gArI5zWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "from os import walk\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbrygJa85zWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path= \"./pheme-rnr-dataset\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAt2C1Y_5zWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_html_tags(text):\n",
        "    \"\"\"Remove html tags from a string\"\"\"\n",
        "    import re\n",
        "    clean = re.compile('<.*?>')\n",
        "    return re.sub(clean, '', text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6EAh0GE5zWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def listofcontent(path):\n",
        "    folder = []\n",
        "    files = []\n",
        "    for (dirpath, dirnames, filenames) in walk(path):\n",
        "        folder.extend(dirnames)\n",
        "        files.extend(filenames)\n",
        "        break\n",
        "    return folder,files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSQPcAGB5zWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rfolder,rfile = listofcontent(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6tyaU2o5zWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = {\"text\":[],\"in_reply_to_status_id\":[],\"id\":[],\"favorite_count\":[],\"source\":[],\"in_reply_to_screen_name\":[],\"id_str\":[],\"retweet_count\":[],\"in_reply_to_user_id\":[],\"favorited\":[],\"lang\":[],\"created_at\":[],\"in_reply_to_status_id_str\":[],\"place_name\":[],\"country\":[],\"place_type\":[],\"user_id\":[],\"user_verified\":[],\"user_followers_count\":[],\"user_friends_count\":[],\"user_screen_name\":[],\"user_favourites_count\":[],\"user_name\":[],\"user_created_at\":[],\"user_is_translator\":[],\"user_mentions\":[],\"hashtags\":[],\"urls\":[],\"tweet_type\":[],\"label\":[]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhgniTq45zWU",
        "colab_type": "code",
        "outputId": "b15b1dc6-15ae-4b96-9fb1-d6c8f74afc21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "index = 0\n",
        "for folder in tqdm(rfolder):\n",
        "    subpath = path + \"/\" + folder\n",
        "    subfolder,_ = listofcontent(subpath)\n",
        "    for basefolder in subfolder:\n",
        "        rumpath = subpath + \"/\" + basefolder\n",
        "        rumfolder,_=listofcontent(rumpath)\n",
        "        for _id in rumfolder:\n",
        "            idpath = rumpath + \"/\" + _id\n",
        "            idfolder,_ = listofcontent(idpath)\n",
        "            for action in idfolder:\n",
        "                tweetpath = idpath + \"/\" + action\n",
        "                _,tweetfiles = listofcontent(tweetpath)\n",
        "                for tweetfile in tweetfiles:\n",
        "                    f = open(tweetpath + \"/\" + tweetfile,)\n",
        "                    tweet = json.load(f)\n",
        "                    #used to handle specific set of data\n",
        "                    for header in tweet.keys():\n",
        "                        if header in data.keys():\n",
        "                            data[header].append(tweet[header])\n",
        "                    data[\"user_id\"].append(tweet[\"user\"][\"id_str\"])\n",
        "                    data[\"user_verified\"].append(tweet[\"user\"][\"verified\"])\n",
        "                    data[\"user_followers_count\"].append(tweet[\"user\"][\"followers_count\"])\n",
        "                    data[\"user_friends_count\"].append(tweet[\"user\"][\"friends_count\"])\n",
        "                    data[\"user_screen_name\"].append(tweet[\"user\"][\"screen_name\"])\n",
        "                    data[\"user_favourites_count\"].append(tweet[\"user\"][\"favourites_count\"])\n",
        "                    data[\"user_created_at\"].append(tweet[\"user\"][\"created_at\"])\n",
        "                    data[\"user_is_translator\"].append(tweet[\"user\"][\"is_translator\"])\n",
        "                    if tweet[\"place\"] == None:\n",
        "                        data[\"place_name\"].append(\"\")\n",
        "                        data[\"country\"].append(\"\")\n",
        "                        data[\"place_type\"].append(\"\")\n",
        "                    else:    \n",
        "                        data[\"place_name\"].append(tweet[\"place\"][\"full_name\"])\n",
        "                        data[\"country\"].append(tweet[\"place\"][\"country\"])\n",
        "                        data[\"place_type\"].append(tweet[\"place\"][\"place_type\"])\n",
        "                    if tweet[\"entities\"] == None:\n",
        "                        data[\"user_mentions\"].append(\"\")\n",
        "                        data[\"hashtags\"].append(\"\")\n",
        "                        data[\"urls\"].append(0)\n",
        "                    else:\n",
        "                        temp = [i[\"id_str\"]+\",\"+i[\"screen_name\"] for i in tweet[\"entities\"][\"user_mentions\"]]\n",
        "                        data[\"user_mentions\"].append(len(join(temp)))\n",
        "                        temp = [i[\"text\"] for i in tweet[\"entities\"][\"hashtags\"]]\n",
        "                        data[\"hashtags\"].append(len(temp))\n",
        "                        data[\"urls\"].append(len(tweet[\"entities\"][\"urls\"]))\n",
        "                    data[\"tweet_type\"].append(action)\n",
        "                    data[\"label\"].append(basefolder)                    "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNdo0od2PR_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame.from_dict(data)\n",
        "df.to_csv(r'.\\pheme-rumor-dataset.csv', index = True,header = True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}